{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/23 17:04:25 WARN Utils: Your hostname, codespaces-201091 resolves to a loopback address: 127.0.0.1; using 172.16.5.4 instead (on interface eth0)\n",
      "24/04/23 17:04:25 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/04/23 17:04:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.master(\"local[1]\") \\\n",
    "                    .appName('test') \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "static = spark.read.json('../data/activity-data/')\n",
    "dataSchema = static.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1).json('../data/activity-data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streaming.isStreaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Arrival_Time: long (nullable = true)\n",
      " |-- Creation_Time: long (nullable = true)\n",
      " |-- Device: string (nullable = true)\n",
      " |-- Index: long (nullable = true)\n",
      " |-- Model: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- gt: string (nullable = true)\n",
      " |-- x: double (nullable = true)\n",
      " |-- y: double (nullable = true)\n",
      " |-- z: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming.writeStream.trigger(once=True).format(\"console\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Всякое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = f.lit('2024-02-19 12:47:36') < (f.lit('2024-02-09 13:00:00') + f.expr('INTERVAL 23 HOURS'))\n",
    "b = f.expr('from_utc_timestamp(\"2024-02-19 12:47:36\", \"Asia/Jerusalem\")')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------------+--------+-----+------+----+-----+-----------+------------+------------+-----+-------------------+\n",
      "| Arrival_Time|      Creation_Time|  Device|Index| Model|User|   gt|          x|           y|           z|    a|                  b|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+-----------+------------+------------+-----+-------------------+\n",
      "|1424686735090|1424686733090638193|nexus4_1|   18|nexus4|   g|stand|3.356934E-4|-5.645752E-4|-0.018814087|false|2024-02-19 14:47:36|\n",
      "+-------------+-------------------+--------+-----+------+----+-----+-----------+------------+------------+-----+-------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static.withColumn('a', a).withColumn('b', b).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "activityCounts = streaming.groupBy(\"gt\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/04 09:13:19 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-a39174c3-93bc-4d83-ab54-a91e47e3a2a4. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/04 09:13:19 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 25:========================================>             (150 + 1) / 200]\r"
     ]
    }
   ],
   "source": [
    "activityQuery = activityCounts.writeStream.queryName(\"activity_counts\").format(\"memory\").outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "\n",
    "for x in range(5):\n",
    "    spark.sql(\"SELECT * FROM activity_counts\").show()\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr\n",
    "\n",
    "simpleTransform = streaming.withColumn(\"stairs\", expr(\"gt like '%stairs%'\"))\\\n",
    "    .where(\"stairs\")\\\n",
    "    .where(\"gt is not null\")\\\n",
    "    .select(\"gt\", \"model\", \"arrival_time\", \"creation_time\")\\\n",
    "    .writeStream\\\n",
    "    .queryName(\"simple_transform\")\\\n",
    "    .format(\"memory\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(5):\n",
    "    spark.sql(\"SELECT * FROM simple_transform\").show()\n",
    "    sleep(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Разовый прогон для дебага"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/05 11:49:02 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-23d839c7-d38d-4b69-946d-69d9b200a73d. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/05 11:49:02 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/04/05 11:49:02 WARN MicroBatchExecution: The read limit MaxFiles: 1 for FileStreamSource[file:/workspaces/Spark-The-Definitive-Guide/data/activity-data] is ignored when Trigger.Once is used.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "distDevice = streaming.select('Device').distinct().writeStream.trigger(once=True).queryName('dist_device').format('memory').outputMode(\"append\").start()\n",
    "distDevice.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+\n",
      "|  Device|\n",
      "+--------+\n",
      "|nexus4_1|\n",
      "|nexus4_2|\n",
      "+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM dist_device\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/05 11:15:56 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-7b5ac24f-a735-4d67-add5-a40353c0d833. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/05 11:15:56 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f524c7d74c0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/05 11:15:57 WARN MicroBatchExecution: The read limit MaxFiles: 1 for FileStreamSource[file:/workspaces/Spark-The-Definitive-Guide/data/activity-data] is ignored when Trigger.Once is used.\n",
      "[Stage 8:====================================>                  (134 + 1) / 200]\r"
     ]
    }
   ],
   "source": [
    "streaming.select('Device').distinct().groupBy('Device').count().writeStream.trigger(once=True).format(\"console\").outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Пробую"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Arrival_Time',\n",
       " 'Creation_Time',\n",
       " 'Device',\n",
       " 'Index',\n",
       " 'Model',\n",
       " 'User',\n",
       " 'gt',\n",
       " 'x',\n",
       " 'y',\n",
       " 'z']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "streaming.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/05 14:33:46 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-662d21dc-bb4e-428a-bb72-bba8b6cfed97. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/05 14:33:46 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/04/05 14:33:47 WARN MicroBatchExecution: The read limit MaxFiles: 1 for FileStreamSource[file:/workspaces/Spark-The-Definitive-Guide/data/activity-data] is ignored when Trigger.Once is used.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filtered = streaming.select('Device', 'gt').distinct()\n",
    "a = filtered.writeStream.trigger(once=True).queryName('a').format('memory').outputMode(\"append\").start()\n",
    "a.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|  Device|        gt|\n",
      "+--------+----------+\n",
      "|nexus4_1|      bike|\n",
      "|nexus4_1|       sit|\n",
      "|nexus4_2|       sit|\n",
      "|nexus4_2|  stairsup|\n",
      "|nexus4_1|stairsdown|\n",
      "|nexus4_2|      null|\n",
      "|nexus4_1|  stairsup|\n",
      "|nexus4_2|      walk|\n",
      "|nexus4_1|      null|\n",
      "|nexus4_1|     stand|\n",
      "|nexus4_1|      walk|\n",
      "|nexus4_2|      bike|\n",
      "|nexus4_2|stairsdown|\n",
      "|nexus4_2|     stand|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from a').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/05 14:35:48 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-6102e6b9-4c75-40eb-82ea-76a9efcd3270. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/05 14:35:48 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n",
      "24/04/05 14:35:49 WARN MicroBatchExecution: The read limit MaxFiles: 1 for FileStreamSource[file:/workspaces/Spark-The-Definitive-Guide/data/activity-data] is ignored when Trigger.Once is used.\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "filtered2 = filtered.filter('Device = \"nexus4_2\"')\n",
    "b = filtered2.writeStream.trigger(once=True).queryName('b').format('memory').outputMode(\"append\").start()\n",
    "b.awaitTermination()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|  Device|        gt|\n",
      "+--------+----------+\n",
      "|nexus4_2|       sit|\n",
      "|nexus4_2|  stairsup|\n",
      "|nexus4_2|      null|\n",
      "|nexus4_2|      walk|\n",
      "|nexus4_2|      bike|\n",
      "|nexus4_2|stairsdown|\n",
      "|nexus4_2|     stand|\n",
      "+--------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from b').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flight Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "static = spark.read.json('../data/flight-data/json/2010-summary.json')\n",
    "dataSchema = static.schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming = spark.readStream.schema(dataSchema).option(\"maxFilesPerTrigger\", 1).json('../data/flight-data/json/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "streaming.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A.number where origin is \"Ireland\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = streaming.filter(\"ORIGIN_COUNTRY_NAME = 'Ireland'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/05 12:33:42 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-00020e41-cda2-4497-aef5-77ec123c16bc. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/05 12:33:42 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f23d50d24d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/05 12:33:43 WARN MicroBatchExecution: The read limit MaxFiles: 1 for FileStreamSource[file:/workspaces/Spark-The-Definitive-Guide/data/flight-data/json] is ignored when Trigger.Once is used.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Ireland|  344|\n",
      "|    United States|            Ireland|  264|\n",
      "|    United States|            Ireland|  268|\n",
      "|    United States|            Ireland|  266|\n",
      "|    United States|            Ireland|  252|\n",
      "|    United States|            Ireland|  291|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a.writeStream.trigger(once=True).format(\"console\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1 = a.groupBy('ORIGIN_COUNTRY_NAME', 'DEST_COUNTRY_NAME').sum('count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/05 12:37:08 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-892352dc-226f-4267-8d26-5ac14d3d82b2. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/05 12:37:08 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f23c2fd56c0>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/05 12:37:08 WARN MicroBatchExecution: The read limit MaxFiles: 1 for FileStreamSource[file:/workspaces/Spark-The-Definitive-Guide/data/flight-data/json] is ignored when Trigger.Once is used.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------------------+-----------------+----------+\n",
      "|ORIGIN_COUNTRY_NAME|DEST_COUNTRY_NAME|sum(count)|\n",
      "+-------------------+-----------------+----------+\n",
      "|            Ireland|    United States|      1685|\n",
      "+-------------------+-----------------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "a1.writeStream.trigger(once=True).format(\"console\").outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. Top Origin on cnt (without USA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/05 13:08:48 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-c5e4baf1-34db-4b0c-a32a-afc7009953e4. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/05 13:08:48 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pyspark.sql.streaming.query.StreamingQuery at 0x7f23c2e501f0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/04/05 13:08:48 WARN MicroBatchExecution: The read limit MaxFiles: 1 for FileStreamSource[file:/workspaces/Spark-The-Definitive-Guide/data/flight-data/json] is ignored when Trigger.Once is used.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Batch: 0\n",
      "-------------------------------------------\n",
      "+-------------------+-----------------+\n",
      "|ORIGIN_COUNTRY_NAME|destination_total|\n",
      "+-------------------+-----------------+\n",
      "|             Canada|            49695|\n",
      "|             Mexico|            38225|\n",
      "|     United Kingdom|            10358|\n",
      "|              Japan|             8643|\n",
      "|            Germany|             8380|\n",
      "| Dominican Republic|             7194|\n",
      "|        The Bahamas|             5775|\n",
      "|             France|             5290|\n",
      "|           Colombia|             4981|\n",
      "|        South Korea|             4253|\n",
      "|            Jamaica|             4087|\n",
      "|              China|             4021|\n",
      "|        Netherlands|             3779|\n",
      "|             Brazil|             3427|\n",
      "|         Costa Rica|             3299|\n",
      "|        El Salvador|             2832|\n",
      "|              Spain|             2550|\n",
      "|             Panama|             2480|\n",
      "|              Italy|             2413|\n",
      "|           Honduras|             2364|\n",
      "+-------------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import desc, asc\n",
    "\n",
    "c = streaming.filter(\"ORIGIN_COUNTRY_NAME <> 'United States'\").select(\"ORIGIN_COUNTRY_NAME\", \"count\") \\\n",
    ".groupBy(\"ORIGIN_COUNTRY_NAME\").sum(\"count\").withColumnRenamed(\"sum(count)\", \"destination_total\") \\\n",
    ".sort(desc(\"destination_total\"))\n",
    "\n",
    "c.writeStream.trigger(once=True).format(\"console\").outputMode(\"complete\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Booking data covid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{\n",
    "    \"city\": \"Amsterdam\",\n",
    "    \"country\": \"Netherlands\",\n",
    "    \"timestamp\": \"2020-01-01T00:00:00\",\n",
    "    \"new_cases\": 1200,\n",
    "    \"reporter\": \"AMS01\"\n",
    "}, {\n",
    "    \"city\": \"Amsterdam\",\n",
    "    \"country\": \"Netherlands\",\n",
    "    \"timestamp\": \"2020-01-01T10:00:00\",\n",
    "    \"new_cases\": 1300,\n",
    "    \"reporter\": \"AMS02\"\n",
    "}, {\n",
    "    \"city\": \"Amsterdam\",\n",
    "    \"country\": \"Netherlands\",\n",
    "    \"timestamp\": \"2020-01-01T12:00:00\",\n",
    "    \"new_cases\": 1400,\n",
    "    \"reporter\": \"AMS01\"\n",
    "}, {\n",
    "    \"city\": \"Utrecht\",\n",
    "    \"country\": \"Netherlands\",\n",
    "    \"timestamp\": \"2020-01-01T00:00:00\",\n",
    "    \"new_cases\": 100,\n",
    "    \"reporter\": \"UTR01\"\n",
    "}, {\n",
    "    \"city\": \"Maastricht\",\n",
    "    \"country\": \"Netherlands\",\n",
    "    \"timestamp\": \"2020-01-04T00:00:00\",\n",
    "    \"new_cases\": 35,\n",
    "    \"reporter\": \"MAAS01\"\n",
    "}, {\n",
    "    \"city\": \"Maastricht\",\n",
    "    \"country\": \"Netherlands\",\n",
    "    \"timestamp\": \"2020-01-05T00:00:00\",\n",
    "    \"new_cases\": 35,\n",
    "    \"reporter\": \"MAAS01\"\n",
    "}, {\n",
    "    \"city\": \"Paris\",\n",
    "    \"country\": \"France\",\n",
    "    \"timestamp\": \"2020-01-05T00:00:00\",\n",
    "    \"new_cases\": 312,\n",
    "    \"reporter\": \"PAR01\"\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         city      country            timestamp  new_cases reporter\n",
      "1   Amsterdam  Netherlands  2020-01-01T00:00:00       1200    AMS01\n",
      "2   Amsterdam  Netherlands  2020-01-01T10:00:00       1300    AMS02\n",
      "3   Amsterdam  Netherlands  2020-01-01T12:00:00       1400    AMS01\n",
      "4     Utrecht  Netherlands  2020-01-01T00:00:00        100    UTR01\n",
      "5  Maastricht  Netherlands  2020-01-04T00:00:00         35   MAAS01\n",
      "6  Maastricht  Netherlands  2020-01-05T00:00:00         35   MAAS01\n",
      "7       Paris       France  2020-01-05T00:00:00        312    PAR01\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "  \n",
    "df = pd.DataFrame.from_records(data,index=['1', '2', '3', '4', '5', '6', '7']) \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkDF=spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You may only use the following given operators of a hypothetical distributed stream processing framework:\n",
    "\n",
    "filter(filterFunc: T = > Boolean): Stream[T]\n",
    "\n",
    "Return a new stream of type T containing only the elements that satisfy a predicate(a function of type T that returns either True or False).\n",
    "\n",
    "map[U](mapFunc: (T) = > U): Stream[U]\n",
    "\n",
    "Return a new stream of type U by applying a function to all elements of the input stream of type T.\n",
    "\n",
    "reduceByKey(reduceFunc: (V, V) = > V): Stream[(K, V)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find the list of Dutch cities present in the stream."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Amsterdam', 'Utrecht', 'Maastricht']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkDF.filter('country = \"Netherlands\"').rdd \\\n",
    "    .map(lambda x: (x[\"city\"], x)) \\\n",
    "    .reduceByKey(lambda x, y: x) \\\n",
    "    .map(lambda x: x[0]) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.The daily number of cases per reporter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------+---------------+\n",
      "|                day|reporter|new_cases_total|\n",
      "+-------------------+--------+---------------+\n",
      "|2020-01-01 00:00:00|   AMS01|           2600|\n",
      "|2020-01-04 00:00:00|  MAAS01|             35|\n",
      "|2020-01-01 00:00:00|   UTR01|            100|\n",
      "|2020-01-01 00:00:00|   AMS02|           1300|\n",
      "|2020-01-05 00:00:00|   PAR01|            312|\n",
      "|2020-01-05 00:00:00|  MAAS01|             35|\n",
      "+-------------------+--------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkDF.select('new_cases', 'reporter', F.date_trunc('day', 'timestamp').alias('day')).groupBy('day', 'reporter').sum('new_cases').withColumnRenamed(\"sum(new_cases)\", \"new_cases_total\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('AMS01', '2020-01-01'), 2600),\n",
       " (('AMS02', '2020-01-01'), 1300),\n",
       " (('UTR01', '2020-01-01'), 100),\n",
       " (('MAAS01', '2020-01-04'), 35),\n",
       " (('MAAS01', '2020-01-05'), 35),\n",
       " (('PAR01', '2020-01-05'), 312)]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkDF.rdd \\\n",
    "    .map(lambda x: ((x['reporter'], x['timestamp'][:10]), x['new_cases'])) \\\n",
    "    .reduceByKey(lambda x, y: x + y) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B.Total number of cases per city and date."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('Amsterdam', '2020-01-01'), ('Netherlands', 3900)),\n",
       " (('Utrecht', '2020-01-01'), ('Netherlands', 100)),\n",
       " (('Maastricht', '2020-01-04'), ('Netherlands', 35)),\n",
       " (('Maastricht', '2020-01-05'), ('Netherlands', 35)),\n",
       " (('Paris', '2020-01-05'), ('France', 312))]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkDF.rdd \\\n",
    "    .map(lambda x: ((x['city'], x['timestamp'][:10]), (x['country'], x['new_cases']))) \\\n",
    "    .reduceByKey(lambda x, y: (x[0], x[1] + y[1])) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. For each day, return the top most infected city per country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('Netherlands', '2020-01-01'), ('Amsterdam', 3900)),\n",
       " (('Netherlands', '2020-01-04'), ('Maastricht', 35)),\n",
       " (('Netherlands', '2020-01-05'), ('Maastricht', 35)),\n",
       " (('France', '2020-01-05'), ('Paris', 312))]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkDF.rdd \\\n",
    "    .map(lambda x: ((x['country'], x['timestamp'][:10], x['city']), x['new_cases'])) \\\n",
    "    .reduceByKey(lambda x, y: x + y) \\\n",
    "    .map(lambda x: ((x[0][0], x[0][1]), (x[0][2], x[1]))) \\\n",
    "    .reduceByKey(lambda x, y: x if x[1] > y[1] else y) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. For each month, return the top 5 infected cities per country."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('Netherlands', '2020-01-01'), [('Amsterdam', 3900), ('Utrecht', 100)]),\n",
       " (('France', '2020-01-01'), [('Paris', 312)])]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "# 2020-01-01T00:00:00\n",
    "\n",
    "sparkDF.rdd \\\n",
    "    .map(lambda x: ((x['country'], datetime.strptime(x['timestamp'], '%Y-%m-%dT%H:%M:%S').replace(day=1).strftime(\"%Y-%m-%d\"), x['city']), x['new_cases'])) \\\n",
    "    .reduceByKey(lambda x, y: x + y) \\\n",
    "    .map(lambda x: ((x[0][0], x[0][1]), [(x[0][2], x[1])])) \\\n",
    "    .reduceByKey(lambda x, y: x + y) \\\n",
    "    .mapValues(lambda x: sorted(x, key = lambda x: x[1], reverse=True)[:2]) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n",
      "|max_new_cases|min_new_cases|\n",
      "+-------------+-------------+\n",
      "|1400         |100          |\n",
      "+-------------+-------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "sparkDF.agg(\n",
    "    F.max(F.col(\"new_cases\")).alias(\"max_new_cases\"),\n",
    "    F.min(F.col(\"new_cases\")).alias(\"min_new_cases\")\n",
    ").show(5, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Booking data hotel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{\n",
    "    \"hotel_id\": 23,\n",
    "    \"hotel_area_code\": \"IT\",\n",
    "    \"unique_user_id\": \"user_1\",\n",
    "    \"is_user_logged_in\": True\n",
    "}, {\n",
    "    \"hotel_id\": 23,\n",
    "    \"hotel_area_code\": \"IT\",\n",
    "    \"unique_user_id\": \"user_2\",\n",
    "    \"is_user_logged_in\": True\n",
    "}, {\n",
    "    \"hotel_id\": 23,\n",
    "    \"hotel_area_code\": \"IT\",\n",
    "    \"unique_user_id\": \"user_1\",\n",
    "    \"is_user_logged_in\": True\n",
    "}, {\n",
    "    \"hotel_id\": 42,\n",
    "    \"hotel_area_code\": \"IT\",\n",
    "    \"unique_user_id\": \"user_3\",\n",
    "    \"is_user_logged_in\": True\n",
    "}, {\n",
    "    \"hotel_id\": 42,\n",
    "    \"hotel_area_code\": \"IT\",\n",
    "    \"unique_user_id\": \"user_4\",\n",
    "    \"is_user_logged_in\": False\n",
    "}, {\n",
    "    \"hotel_id\": 88,\n",
    "    \"hotel_area_code\": \"FR\",\n",
    "    \"unique_user_id\": \"user_1\",\n",
    "    \"is_user_logged_in\": True\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hotel_id hotel_area_code unique_user_id  is_user_logged_in\n",
      "0        23              IT         user_1               True\n",
      "1        23              IT         user_2               True\n",
      "2        23              IT         user_1               True\n",
      "3        42              IT         user_3               True\n",
      "4        42              IT         user_4              False\n",
      "5        88              FR         user_1               True\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "  \n",
    "df = pd.DataFrame.from_records(data) \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkDF=spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For each area code the hotel with the higher number of views by logged in users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('IT', (23, 3)), ('FR', (88, 1))]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkDF.filter('is_user_logged_in == True').rdd \\\n",
    "    .map(lambda x: ((x['hotel_area_code'], x['hotel_id']), (1))) \\\n",
    "    .reduceByKey(lambda x, y: x + y) \\\n",
    "    .map(lambda x: (x[0][0], (x[0][1], x[1]))) \\\n",
    "    .reduceByKey(lambda x, y: x if x[1] > y[1] else y) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## * Distinct area codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('IT', ''), ('FR', '')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkDF.rdd \\\n",
    "    .map(lambda x: (x['hotel_area_code'], '')) \\\n",
    "    .reduceByKey(lambda x, y: x) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Booking tasks from medium/leetcode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Map Reduce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     0      1  2\n",
      "0  a-b  data1  1\n",
      "1  a-c  data2  1\n",
      "2  a-b  data3  1\n"
     ]
    }
   ],
   "source": [
    "data = [\n",
    "    (\"a-b\", \"data1\", 1),\n",
    "    (\"a-c\", \"data2\", 1),\n",
    "    (\"a-b\", \"data3\", 1)\n",
    "    ]\n",
    "\n",
    "import pandas as pd  \n",
    "  \n",
    "df = pd.DataFrame.from_records(data) \n",
    "print(df)\n",
    "\n",
    "sparkDF=spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('a-b', (['data1', 'data3'], 2)), ('a-c', (['data2'], 1))]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  (\"a-b\", [\"data1\", \"data3\"], 2) (\"a-c\", [\"data2\"], 1)\n",
    "sparkDF.rdd \\\n",
    "    .map(lambda x: (x[0],([x[1]], x[2]))) \\\n",
    "    .reduceByKey(lambda x, y: (x[0] + y[0], x[1] + y[1])) \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table Build Order\n",
    "As part of a job submission to a server, that is responsible for orchestrating the creation of tables for consumption by reports, a piece of metadata is submitted that declares the dependencies of the various tables for the given job.It is the responsibility of the server to make sure that a correct build order is chosen and executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dependencies = {\n",
    "    \"TableC\": [\"TableD\"],\n",
    "    \"TableA\": [\"TableB\", \"TableC\"],\n",
    "    \"TableE\": [\"TableB\"],\n",
    "    \"TableD\": [\"TableB\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TableB', 'TableD', 'TableC', 'TableA', 'TableE']\n"
     ]
    }
   ],
   "source": [
    "ans = []\n",
    "tables = set()\n",
    "# get set of all differen tables\n",
    "for k, v in dependencies.items():\n",
    "    tables.add(k)\n",
    "    for t in v:\n",
    "        tables.add(t)\n",
    "\n",
    "# add tables withot dependencies to the dependencies dict\n",
    "for table in tables:\n",
    "    if table not in dependencies.keys():\n",
    "        dependencies[table] = []\n",
    "\n",
    "# find root\n",
    "non_root = []\n",
    "for k, v in dependencies.items():\n",
    "    non_root += list(v)\n",
    "roots = [k for k, v in dependencies.items() if k not in non_root]\n",
    "\n",
    "# dfs\n",
    "def dfs(table):\n",
    "    for dep in dependencies[table]:\n",
    "        dfs(dep)\n",
    "    if table not in ans:\n",
    "        ans.append(table)\n",
    "\n",
    "# start\n",
    "for root in roots:\n",
    "    dfs(root)  \n",
    "print(ans)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge hotels\n",
    "Merge HotelRoom object when highPrice + 1 = lowPrice of other object\n",
    " - Ex.1  [1,2], [3,4] ==> [1,4]\n",
    " - Ex.2  [1,5],[6,9] ==> [1,9]\n",
    " - Ex.3 [1,5], [14, 17], [6,9], [10,13] ==> [1,17]\n",
    " - Ex.3 [1,5], [14, 17], [6,9], [10,13], [4,7], [8,12] ==> [1,17], [4,12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1, 2], [3, 4]]\n"
     ]
    }
   ],
   "source": [
    "arr_obj = [[1,2], [3,4]]\n",
    "arr_obj.sort(key=lambda x: x[0])\n",
    "print(arr_obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2]\n",
      "ans [[1, 2]]\n",
      "[3, 4]\n",
      "ans [[1, 4]]\n",
      "[[1, 4]]\n"
     ]
    }
   ],
   "source": [
    "ans = []\n",
    "for cur in arr_obj:\n",
    "    print(cur)\n",
    "    merged = False\n",
    "    for i in range(len(ans)):\n",
    "        if ans[i][1] + 1 == cur[0]:\n",
    "            ans[i][1] = cur[1]\n",
    "            merged = True\n",
    "            break\n",
    "        if ans[i][0] == cur[1] + 1:\n",
    "            ans[i][0] = cur[0]\n",
    "            merged = True\n",
    "            break\n",
    "    if not merged:\n",
    "        ans.append(cur)\n",
    "\n",
    "    print('ans', ans)\n",
    "\n",
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backspaceing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = 'abc#def##'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abd\n"
     ]
    }
   ],
   "source": [
    "sa = []\n",
    "for c in s:\n",
    "    if c != '#':\n",
    "        sa.append(c)\n",
    "    else:\n",
    "        sa.pop()\n",
    "print(''.join(sa))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check-in/out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = {\n",
    "\n",
    "\t176 : \t[\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"price\" : 120,\n",
    "\t\t\t\t\t\"features\" : [\"breakfast\", \"refundable\"],\n",
    "\t\t\t\t\t\"availbility\" : 5\n",
    "\t\t\t\t}\n",
    "\t\t\t],\n",
    "\t177 : \t[\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"price\" : 130,\n",
    "\t\t\t\t\t\"features\" : [\"breakfast\"],\n",
    "\t\t\t\t\t\"availbility\" : 1\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"price\" : 140,\n",
    "\t\t\t\t\t\"features\" : [\"breakfast\", \"refundable\", \"wifi\"],\n",
    "\t\t\t\t\t\"availbility\" : 3\n",
    "\t\t\t\t}\n",
    "\t\t\t],\n",
    "\t178 : \t[\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"price\" : 130,\n",
    "\t\t\t\t\t\"features\" : [\"breakfast\"],\n",
    "\t\t\t\t\t\"availbility\" : 2\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"price\" : 140,\n",
    "\t\t\t\t\t\"features\" : [\"breakfast\", \"refundable\", \"wifi\"],\n",
    "\t\t\t\t\t\"availbility\" : 10\n",
    "\t\t\t\t}\n",
    "\t\t\t]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = {\n",
    "\t\"checkin\" : 176,\n",
    "\t\"checkout\" : 178,\n",
    "\t\"features\" : [\"breakfast\"],\n",
    "\t\"rooms\"    : 1\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Expected output\n",
    "\n",
    "[\n",
    "\t{\n",
    "\t\t\"price\" : 250,\n",
    "\t\t\"features\" : [\"breakfast\"],\n",
    "\t\t\"availbility\" : 1\n",
    "\t},\n",
    "\t{\n",
    "\t\t\"price\" : 260,\n",
    "\t\t\"features\" : [\"breakfast\", \"refundable\"],\n",
    "\t\t\"availbility\" : 3\n",
    "\t}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = []\n",
    "\n",
    "# for i in range(input['checkin'], input['checkout'] + 1):\n",
    "def dfs(i, cur):\n",
    "    if i == input['checkout']:\n",
    "        ans.append(cur)\n",
    "        return\n",
    "    for option in d[i]:\n",
    "        if all(feature in option['features'] for feature in input['features']) and option['availbility'] >= input['rooms']:\n",
    "            if cur:\n",
    "                new_cur = {}\n",
    "                new_cur['price'] = cur['price'] + option['price']\n",
    "                new_cur['features'] = list(set(cur['features']) & set(option['features']))\n",
    "                new_cur['availbility'] = min(cur['availbility'], option['availbility'])\n",
    "            else:\n",
    "                new_cur = {}\n",
    "                new_cur['price'] = option['price']\n",
    "                new_cur['features'] = set(option['features'])\n",
    "                new_cur['availbility'] = option['availbility']\n",
    "            dfs(i+1, new_cur)\n",
    "\n",
    "dfs(input['checkin'], {})\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'price': 250, 'features': ['breakfast'], 'availbility': 1}, {'price': 260, 'features': ['breakfast', 'refundable'], 'availbility': 3}]\n"
     ]
    }
   ],
   "source": [
    "print(ans)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Given a stream of rows\n",
    "in the following format, find the cities which has 3 or more hotels with same name\n",
    "hotelId, hotel Name, City Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [{\n",
    "    \"hotel_id\": 1,\n",
    "    \"hotel_name\": \"A\",\n",
    "    \"city\": \"Amsterdam\",\n",
    "}, {\n",
    "    \"hotel_id\": 2,\n",
    "    \"hotel_name\": \"B\",\n",
    "    \"city\": \"Amsterdam\",\n",
    "}, {\n",
    "    \"hotel_id\": 3,\n",
    "    \"hotel_name\": \"C\",\n",
    "    \"city\": \"Amsterdam\",\n",
    "}, {\n",
    "    \"hotel_id\": 4,\n",
    "    \"hotel_name\": \"A\",\n",
    "    \"city\": \"Amsterdam\",\n",
    "}, {\n",
    "    \"hotel_id\": 4,\n",
    "    \"hotel_name\": \"A\",\n",
    "    \"city\": \"Amsterdam\",\n",
    "}, {\n",
    "    \"hotel_id\": 5,\n",
    "    \"hotel_name\": \"A\",\n",
    "    \"city\": \"Amsterdam\",\n",
    "}, {\n",
    "    \"hotel_id\": 6,\n",
    "    \"hotel_name\": \"A\",\n",
    "    \"city\": \"Paris\",\n",
    "}, {\n",
    "    \"hotel_id\": 7,\n",
    "    \"hotel_name\": \"A\",\n",
    "    \"city\": \"Paris\",\n",
    "}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   hotel_id hotel_name       city\n",
      "0         1          A  Amsterdam\n",
      "1         2          B  Amsterdam\n",
      "2         3          C  Amsterdam\n",
      "3         4          A  Amsterdam\n",
      "4         4          A  Amsterdam\n",
      "5         5          A  Amsterdam\n",
      "6         6          A      Paris\n",
      "7         7          A      Paris\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd  \n",
    "  \n",
    "df = pd.DataFrame.from_records(data) \n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkDF=spark.createDataFrame(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[(('Amsterdam', 'A'), 3),\n",
       " (('Amsterdam', 'B'), 1),\n",
       " (('Amsterdam', 'C'), 1),\n",
       " (('Paris', 'A'), 1)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sparkDF.rdd \\\n",
    "    .map(lambda x: ((x['city'], x['hotel_name']), 1)) \\\n",
    "    .reduceByKey(lambda x, y: x + y) \\\n",
    "    .filter() \\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+---+\n",
      "|     city|hotel_name|cnt|\n",
      "+---------+----------+---+\n",
      "|    Paris|         A|  2|\n",
      "|Amsterdam|         A|  3|\n",
      "+---------+----------+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sparkDF.groupBy(['city', 'hotel_name']) \\\n",
    "    .agg(f.countDistinct('hotel_id').alias('cnt')) \\\n",
    "    .filter('cnt >= 2') \\\n",
    "    .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RollUp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+----+\n",
      "| name| age| name| age|\n",
      "+-----+----+-----+----+\n",
      "| NULL|NULL| NULL|NULL|\n",
      "|Alice|NULL|Alice|NULL|\n",
      "|Alice|   2|Alice|   2|\n",
      "|Alice|   7|Alice|   7|\n",
      "|  Bon|NULL|  Bon|NULL|\n",
      "|  Bon|   5|  Bon|   5|\n",
      "+-----+----+-----+----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([(2, \"Alice\"), (7, \"Alice\"), (5, \"Bon\")], schema=[\"age\", \"name\"])\n",
    "df.rollup(\"name\", \"age\").agg(f.col(\"name\"), f.col(\"age\")).orderBy(\"name\", \"age\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
